\section{Experimental Results}

We test our framework on three commercial buildings, comparing XGBoost~\cite{chen2016xgboost}, LightGBM~\cite{ke2017lightgbm}, CatBoost~\cite{prokhorenkova2018catboost}, Histogram Gradient Boosting, ensembles, and the cascade classifier. All experiments use time-based splits for temporal consistency.

\subsection{Overall Performance}

Figure~\ref{fig:performance} shows classification and regression performance across sites and models.

Classification accuracy spans 70-98\%. Ensembles perform best across all sites. Site A shows the strongest results while Site C proves more difficult due to variable occupancy. The cascade classifier matches top accuracy while offering better interpretability.

Regression varies substantially across sites. Sites A and B achieve RMSE of 0.49-1.15 kW, showing strong accuracy. Site C reaches 5.84-8.06 kW, a 10-fold jump suggesting different building dynamics. Ensembles beat individual models at each site, but all methods struggle equally on Site C. This points to site characteristics rather than model choice.

\begin{figure*}[!htbp]
\centering
\includegraphics[width=0.49\textwidth]{figures/classification_accuracy.png}
\hfill
\includegraphics[width=0.49\textwidth]{figures/regression_performance.png}
\caption{Performance across sites and models. (Left) Classification accuracy. (Right) Regression RMSE in kW. Note: Y-axis scales differ across sites for regression.}
\label{fig:performance}
\end{figure*}

\subsection{Classification Metrics}

\begin{figure*}[!htbp]
\centering
\includegraphics[width=0.95\textwidth]{figures/f1_scores.png}
\vspace{0.3cm}
\includegraphics[width=0.95\textwidth]{figures/g_mean.png}
\caption{Classification metrics across sites. (Top) F1-score breakdown showing macro, micro, and weighted averages. (Bottom) G-Mean scores. The cascade classifier achieves 50-52\% G-Mean, best balancing performance across event types.}
\label{fig:classification_metrics}
\end{figure*}

\subsection{Geometric Mean Performance}

Figure~\ref{fig:classification_metrics} shows F1-scores and geometric mean across sites. F1 uses three averages: macro (unweighted), micro (global), and weighted (frequency-weighted). G-Mean measures per-class recall, designed for imbalanced classification. Unlike F1, G-Mean requires all classes to achieve reasonable recall.

The ensemble reaches 96.6-96.9\% weighted F1 but only 39.3-41.4\% macro F1. LightGBM and XGBoost also exceed 96\% weighted F1. CatBoost and Histogram Gradient Boosting get more balanced macro F1 at 33-35\%. All models exceed 97\% micro F1 from correctly predicting no-events.

The cascade classifier reaches 50-52\% G-Mean across all sites. The ensemble follows at 49.9-50\%. Individual models range from 38.5-51.4\%, with CatBoost showing the most balanced per-class performance. All G-Mean scores stay below 52\%, reflecting difficulty detecting rare events (less than 0.1\% of samples). G-Mean variance across sites stays under 5\%, showing robust generalization.

\subsection{Regression Performance}

On Sites A and B, the ensemble achieves RMSE of 0.495 kW and 0.704 kW, a 7\% gain over LightGBM on Site A. Sample weighting cuts CV-RMSE by 10-12\%. LightGBM beats other individual models, followed by CatBoost and XGBoost. Histogram Gradient Boosting performs worst, likely because its binning strategy suits discrete rather than continuous targets.

Site C is much harder. All models reach 5.84-8.06 kW RMSE, 10Ã— worse than Sites A and B. The ensemble barely helps, dropping error from 6.05-8.06 kW to 5.84 kW. This uniform failure across different algorithms points to site-specific issues (irregular occupancy, complex HVAC, or measurement problems) rather than poor models. The gap shows how building characteristics can matter more than algorithm choice.
